
<img src="https://avatars.githubusercontent.com/u/22390581?s=460&u=e2a3ccb663ae34048a4c2233bb9a530d2de29a9c&v=4" align="right"
     alt="Size Limit logo by Anton Lovchikov" width="300" height="350">

# Sri Sudheera Chitipolu [![](https://img.shields.io/badge/Github-Sudheera96-orange)](https://github.com/sudheera96)

I am Sri Sudheera Chitipolu, currently pursuing Masters in Applied Computer Science, NWMSU, USA. Also working as Graduate Assistant for Computer Science Department. Consistently top performer, result oriented with a positive attitude.

I am certified in 

![AWS Cloud Practitioner](https://raw.githubusercontent.com/sudheera96/badges/main/aws-certified-cloud-practitioner.png) ![IBM Bigdata Fundamentals](https://raw.githubusercontent.com/sudheera96/badges/main/big-data-foundations-level-1.png)   ![H2o.ai](https://raw.githubusercontent.com/sudheera96/badges/main/badge-8779.png) ![kubernetes,containers](https://raw.githubusercontent.com/sudheera96/badges/main/containers-kubernetes-essentials.png)


# PySpark Text processing [![](https://img.shields.io/badge/PySpark-Sudheera96-orange)](https://github.com/sudheera96/pyspark-textprocessing/blob/main/Sri%20Sudheera%20Chitipolu%20-%20Bigdata%20Project%20(1).ipynb)

PySpark Text processing is the project on word count from a website content and visualizing the word count in bar chart and word cloud.

### Used Bigdata skills 
- Databricks Cloud Environment
- Spark Processing Engine
- PySpark API
- Python Programming Language

### Text Resource
_[The Project Gutenberg EBook of Little Women, by Louisa May Alcott](https://www.gutenberg.org/cache/epub/514/pg514.txt)_

### Commands 

```python
import urllib.request
urllib.request.urlretrieve("https://www.gutenberg.org/cache/epub/514/pg514.txt" , "/tmp/littlewomen.txt")
```
```python
dbutils.fs.mv("file:/tmp/littlewomen.txt","dbfs:/data/littlewomen.txt")
```
```python
LittleWomenRawRDD= sc.textFile("dbfs:/data/littlewomen.txt")
```
```python
LittleWomenMessyTokensRDD = LittleWomenRawRDD.flatMap(lambda line: line.lower().strip().split(" "))
```
```python
import re
LittleWomenCleanTokensRDD = LittleWomenMessyTokensRDD.map(lambda letter: re.sub(r'[^A-Za-z]', '', letter))
```
```python
from pyspark.ml.feature import StopWordsRemover
remover = StopWordsRemover()
stopwords = remover.getStopWords()
LittleWomenWordsRDD = LittleWomenCleanTokensRDD.filter(lambda PointLessW: PointLessW not in stopwords)
```
```python
LittleWomenEmptyRemoveRDD = LittleWomenWordsRDD.filter(lambda x: x != "")
```
```python
LittleWomenPairsRDD = LittleWomenEmptyRemoveRDD.map(lambda word: (word,1))
```
```python
LittleWomenWordCountRDD = LittleWomenPairsRDD.reduceByKey(lambda acc, value: acc + value)
```
```python
LittleWomenResults = LittleWomenWordCountRDD.map(lambda x: (x[1], x[0])).sortByKey(False).take(10)
print(LittleWomenResults)
```
```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

source = 'The Project Gutenberg EBook of Little Women, by Louisa May Alcott'
title = 'Top Words in ' + source
xlabel = 'Count'
ylabel = 'Words'

df = pd.DataFrame.from_records(LittleWomenResults, columns =[xlabel, ylabel]) 
plt.figure(figsize=(10,3))
sns.barplot(xlabel, ylabel, data=df, palette="viridis").set_title(title)
```
![Word Count](https://raw.githubusercontent.com/sudheera96/pyspark-textprocessing/main/count.png)
```python
import nltk
import wordcloud
import matplotlib.pyplot as plt

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud

class WordCloudGeneration:
    def preprocessing(self, data):
        # convert all words to lowercase
        data = [item.lower() for item in data]
        # load the stop_words of english
        stop_words = set(stopwords.words('english'))
        # concatenate all the data with spaces.
        paragraph = ' '.join(data)
        # tokenize the paragraph using the inbuilt tokenizer
        word_tokens = word_tokenize(paragraph) 
        # filter words present in stopwords list 
        preprocessed_data = ' '.join([word for word in word_tokens if not word in stop_words])
        print("\n Preprocessed Data: " ,preprocessed_data)
        return preprocessed_data

    def create_word_cloud(self, final_data):
        # initiate WordCloud object with parameters width, height, maximum font size and background color
        # call the generate method of WordCloud class to generate an image
        wordcloud = WordCloud(width=1600, height=800, max_words=10, max_font_size=200, background_color="black").generate(final_data)
        # plt the image generated by WordCloud class
        plt.figure(figsize=(12,10))
        plt.imshow(wordcloud)
        plt.axis("off")
        plt.show()

wordcloud_generator = WordCloudGeneration()
# you may uncomment the following line to use custom input
# input_text = input("Enter the text here: ")
import urllib.request
url = "https://www.gutenberg.org/cache/epub/514/pg514.txt"
request = urllib.request.Request(url)
response = urllib.request.urlopen(request)
input_text = response.read().decode('utf-8')

input_text = input_text.split('.')
clean_data = wordcloud_generator.preprocessing(input_text)
wordcloud_generator.create_word_cloud(clean_data)
```
![Word Cloud](https://raw.githubusercontent.com/sudheera96/pyspark-textprocessing/main/word%20cloud.png)
```python
pip install wordcloud
pip install nltk
nltk.download('popular')
```
```
plt.savefig('LittleWomen_Results.png')
```
## References
- [Word Cloud](https://www.section.io/engineering-education/word-cloud/)
- [String to bytecode](https://stackoverflow.com/questions/4981977/how-to-handle-response-encoding-from-urllib-request-urlopen-to-avoid-typeerr)
